# AWS Simple Storage Service (S3)

Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its e-commerce network. Amazon S3 can store any type of object, which allows uses like storage for Internet applications, backups, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage. 

## Features

- **Object storage:** Store and retrieve any amount of data from anywhere

- **Scalability:** Scale your storage resources up and down to meet fluctuating demands, without upfront investments or resource procurement cycles

- **Security:** Help protect your data using Amazon S3 features

- **Performance:** Retrieve data quickly from Amazon S3 for use with other AWS services

- **Integration:** Select from different storage classes and manage data with lifecycle configuration rules

- **Analytics:** Analyze data and identify patterns using Amazon S3 data access tools and query-in-place services

- **Hybrid Capabilities:** Seamlessly store and retrieve data in AWS with integrated offerings from AWS and AWS Partner Network members

### Object Storage 

Amazon S3 is object storage built to store and retrieve any amount of data from anywhere â€“ web sites and mobile apps, corporate applications, and data from IoT sensors or devices. It is designed to deliver 99.999999999% durability, and scale past trillions of objects worldwide.

Object Storage is a method of storing data as objects, as opposed to other storage architectures like file systems which store data as a file hierarchy, and block storage which stores data as blocks within sectors and tracks. Each object typically includes the data itself, a variable amount of metadata, and a globally unique identifier. Objects can be accessed through a unique URL.

An Object can only be retrieved in its entirety. It is not possible to retrieve only a part of an object.

### Disk Failure 

Amazon S3 is designed to provide 99.999999999% durability of objects over a given year. This durability level corresponds to an average annual expected loss of 0.000000001% of objects. For example, if you store 10,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000,000 years. In addition, Amazon S3 is designed to sustain the concurrent loss of data in two facilities.

To achieve this level of object durability, Amazon S3 redundantly stores your objects on multiple devices across multiple facilities in an Amazon S3 Region. The objects are automatically stored in the Amazon S3 datacenter closest to you.

Erasure Coding is used to store data in multiple locations. Erasure coding is a method of data protection in which data is broken into fragments, expanded and encoded with redundant data pieces and stored across a set of different locations or storage media.

### Scalability

Amazon S3 is designed to scale storage resources up and down to meet fluctuating demands. This means you pay only for the storage you use. There is no minimum fee and no setup cost.

### Security

Amazon S3 supports multiple security features to help protect your data. These include:

- **Authentication mechanisms:** Identity and Access Management (IAM) policies and Bucket Policies

- **Access Control Mechanisms:** Access Control Lists (ACLs) and Bucket Policies

- **Encryption:** Encryption in transit with SSL and at rest with Server-Side Encryption (SSE)

- **Audit capabilities:** AWS CloudTrail to log, monitor, and retain storage API call activities

### Performance

Amazon S3 provides a highly durable storage infrastructure designed for mission-critical and primary data storage. Objects are redundantly stored on multiple devices across multiple facilities in an Amazon S3 Region. To help ensure data durability, Amazon S3 PUT and COPY operations synchronously store your data across multiple facilities before returning SUCCESS. Once stored, Amazon S3 maintains the durability of your objects by quickly detecting and repairing any lost redundancy. Amazon S3 also regularly verifies the integrity of data stored using checksums. If corruption is detected, it is repaired using redundant data. In addition, Amazon S3 calculates checksums on all network traffic to detect corruption of data packets when storing or retrieving data.

### Integration

Amazon S3 provides comprehensive integration with AWS services such as Amazon Athena, AWS Glue, Amazon Kinesis, Amazon CloudWatch, AWS Identity and Access Management, Amazon Macie, AWS Lambda, Amazon S3 Batch Operations, AWS Organizations, AWS PrivateLink, AWS Resource Access Manager, AWS Security Hub, Amazon Redshift, Amazon QuickSight, AWS Snowball, AWS Snowmobile, AWS Systems Manager, AWS Transfer for SFTP, Amazon VPC, and AWS X-Ray to enhance your data management workflows and AWS Partner Network partner services to enhance even further the functionality of Amazon S3 for specific use cases.

### Analytics

Amazon S3 provides management features for you to get insights into your storage usage and activity. You can also use S3 Select to retrieve subsets of object metadata, instead of the entire object, and improve query performance by up to 400%. Amazon S3 also provides inventory reports, which list your objects and their corresponding metadata on a daily or weekly basis for a given S3 bucket. You can use inventory reports to audit and validate the data that you are storing in Amazon S3. Amazon S3 integrates with Amazon Athena to enable ad-hoc analysis of data stored in Amazon S3 using standard SQL. You can use Amazon Athena to query data without having to aggregate or load the data into Athena.

### Hybrid Capabilities

AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure in the cloud. You can use the AWS Management Console to create storage volumes and mount the volumes as iSCSI devices from your on-premises application servers. Data written to these volumes is stored as objects in your Amazon S3 buckets, offering durable, cost-effective storage for data that is not frequently accessed. You can also take point-in-time snapshots of your volumes and write them to Amazon S3 for durable recovery. To transfer large amounts of data to AWS, you can use AWS Snowball appliances to load your data into Amazon S3.

## Storage Classes

Amazon S3 offers a range of storage classes designed for different use cases. These include:

- **S3 Standard:** S3 Standard is the default storage class. It is designed for durability of 99.999999999% of objects across multiple Availability Zones (AZs). It is suitable for a wide variety of use cases including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.

- **S3 Intelligent-Tiering:** S3 Intelligent-Tiering is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It is suitable if you have long-lived data with changing or unknown access patterns, and if you want to optimize storage costs automatically when data access patterns change, without performance impact or operational overhead.

- **S3 Standard-Infrequent Access (S3 Standard-IA):** S3 Standard-IA is designed for durability of 99.999999999% of objects across multiple Availability Zones (AZs). It is suitable for data that is accessed less frequently, but requires rapid access when needed. It is ideal for long-term storage, backups, and as a data store for disaster recovery.

- **S3 One Zone-Infrequent Access (S3 One Zone-IA):** S3 One Zone-IA is designed for durability of 99.999999999% of objects in a single AZ. It is suitable for data that can be recreated if lost. It stores data in a single AZ and costs 20% less than S3 Standard-IA. It is ideal for secondary backup copies of on-premises data and for data that can be easily re-created.

- **S3 Glacier:** S3 Glacier is designed for durability of 99.999999999% of objects over a given year. It is suitable for data archiving and long-term backup of data that will be retained for 7-10 years. It is ideal for digital preservation, regulatory compliance, and data archiving.

- **S3 Glacier Deep Archive:** S3 Glacier Deep Archive is designed for durability of 99.999999999% of objects over a given year. It is suitable for long-term backup and archive of data that will be retained for 10 years or longer. It is ideal for customers who want to replace tape-based backups with a cloud-based, highly durable solution.

- **S3 Outposts:** S3 Outposts is designed to store data on-premises and automatically replicate data to AWS, enabling you to use native AWS services in the AWS Region closest to your on-premises applications. It is suitable for workloads that require low-latency access to data and local data processing.

